# Proyecto de Predicci√≥n de Abandono Escolar

Este proyecto tiene como objetivo predecir el abandono escolar utilizando datos hist√≥ricos de estudiantes desde 2014 a 2024. El flujo abarca desde la limpieza y unificaci√≥n de datos, pasando por el entrenamiento de modelos de machine learning, hasta la implementaci√≥n de una interfaz web para predicci√≥n din√°mica.

---

## üìÅ Estructura de Carpetas y Archivos

```
Proyecto Final Feria/
‚îÇ
‚îú‚îÄ‚îÄ Datasets/
‚îÇ   ‚îî‚îÄ‚îÄ Datasets/Datos a√±o 2014-2024/      # Archivos CSV originales por a√±o
‚îÇ   ‚îî‚îÄ‚îÄ Datasets/Reducidos/                # Archivos CSV reducidos y unificados
‚îÇ
‚îú‚îÄ‚îÄ Desarrollo.ipynb                       # Notebook principal de an√°lisis y modelado
‚îú‚îÄ‚îÄ Previo_unificar_limpieza_de_columnas.py# Script para reducir columnas de los CSV originales
‚îú‚îÄ‚îÄ app_prediccion.py                      # App Streamlit para predicci√≥n interactiva
‚îú‚îÄ‚îÄ modelo_entrenado_XGBoost.pkl           # Modelo XGBoost entrenado y serializado
‚îú‚îÄ‚îÄ columnas_modelo.txt                    # Columnas usadas para el entrenamiento del modelo
‚îú‚îÄ‚îÄ Dashboard.py                           # (Opcional) Dashboard para consulta a API (no REST)
‚îî‚îÄ‚îÄ README.md                              # (Este archivo)
```

---

## üö¶ Flujo y Procedimientos del Proyecto

### 1. **Reducci√≥n y Limpieza Inicial de los Datos**
- **Archivo:** `Previo_unificar_limpieza_de_columnas.py`
- **Funci√≥n:** Lee todos los CSV originales de cada a√±o, selecciona solo las columnas relevantes y guarda una versi√≥n reducida en `Datasets/Reducidos/`.
- **Uso:**  
  ```bash
  python Previo_unificar_limpieza_de_columnas.py
  ```

---

### 2. **Unificaci√≥n y Limpieza Avanzada**
- **Archivo:** `Desarrollo.ipynb`
- **Funci√≥n:**  
  - Une todos los datasets anuales en uno solo, reemplazando informaci√≥n antigua por la m√°s reciente seg√∫n el identificador `MRUN`.
  - Crea la variable `estado` con valores: `abandono_entre_a√±os`, `abandono_en_a√±o`, `cursando`, `egresado` seg√∫n reglas de negocio.
  - Elimina egresados si es necesario.
  - Realiza limpieza de datos, imputaci√≥n de valores, codificaci√≥n de variables categ√≥ricas y balanceo de clases.
  - Entrena y compara varios modelos de clasificaci√≥n (Logistic Regression, Random Forest, Gradient Boosting, XGBoost).
  - Guarda el modelo XGBoost final y las columnas usadas para el entrenamiento.
- **Salida:**  
  - `Datasets/Reducidos/dataset_unificado3_sin_egresados.csv` (u otro nombre seg√∫n versi√≥n)
  - `modelo_entrenado_XGBoost.pkl`
  - `columnas_modelo.txt`

---

### 3. **Entrenamiento y Guardado del Modelo**
- **Archivo:** `Desarrollo.ipynb`
- **Funci√≥n:**  
  - Entrena el modelo XGBoost final con los datos limpios y balanceados.
  - Guarda el modelo entrenado con `joblib.dump`.
  - Guarda las columnas usadas para el entrenamiento en `columnas_modelo.txt`.

---

### 4. **Predicci√≥n Din√°mica v√≠a Interfaz Web**
- **Archivo:** `app_prediccion.py`
- **Funci√≥n:**  
  - Interfaz web con Streamlit para ingresar datos de un estudiante y obtener la probabilidad de abandono.
  - Carga el modelo entrenado y las columnas del modelo.
  - Preprocesa la entrada del usuario igual que en el entrenamiento.
  - Muestra la probabilidad y la predicci√≥n (`Abandona`/`No abandona`).
- **Uso:**  
  ```bash
  streamlit run app_prediccion.py
  ```
  - Accede a [http://localhost:8501](http://localhost:8501) en tu navegador.

---

### 5. **Dashboard de Consulta (Opcional)**
- **Archivo:** `Dashboard.py`
- **Funci√≥n:**  
  - Permite configurar la URL de un endpoint de predicci√≥n (pensado para una API REST, pero Streamlit no expone endpoints REST).
  - √ötil si en el futuro se implementa una API con FastAPI o Flask.

---

## üìã Orden de Ejecuci√≥n Recomendado

1. **Reducir los datasets originales:**  
   Ejecuta `Previo_unificar_limpieza_de_columnas.py` para obtener archivos reducidos.
2. **Unificar, limpiar y preparar los datos:**  
   Trabaja en `Desarrollo.ipynb` para unificar, limpiar, balancear y entrenar modelos.
3. **Guardar el modelo y columnas:**  
   Desde el notebook, guarda el modelo XGBoost y las columnas.
4. **Realizar predicciones din√°micas:**  
   Ejecuta `streamlit run app_prediccion.py` y usa la interfaz web.
5. **(Opcional) Implementar una API REST:**  
   Si necesitas exponer el modelo como API, crea un script con FastAPI o Flask.

---
